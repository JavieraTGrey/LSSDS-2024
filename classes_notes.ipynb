{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbde336-4d75-43cf-a6eb-0c9fcc8f507f",
   "metadata": {},
   "source": [
    "# LSSDS 2024: Notes on clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677fa42-b9d8-4895-8ac0-8f406681af9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## August-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f08faa-92f7-4fad-bea1-e62d74b4028f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Introduction to Data Science by Federica Bianco (University of Delaware fbianco@udel.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff3c71-cf2e-4611-80b7-f8c76772f6aa",
   "metadata": {},
   "source": [
    "What is Data Science?\n",
    "- Domain, Computer Science and Maths meet. A science that it is always growing, finding new problems and solutions, have in mind that learning models or the interpretation data may lead to badly repercussion in real life.\n",
    "\n",
    "Data Science Life Cycle\n",
    "- Scientific Question\n",
    "- Data Collection\n",
    "- Data exploraation (properties extraction)\n",
    "- Data engineering (cleaning, wrangling)\n",
    "- Modeling (ML models, supervised or unsupervised)\n",
    "\n",
    "Data Science Ethics\n",
    "As ML models learn from a dataset, a badly constructed dataset may lead to racist, sexist interpretations (Face recognition algorithms). Also as the models are mostly written in english, ML models may be bias to western culture and languages, keeping in the shadow every other culture or country that doesnt speak english fluently.\n",
    "Finally, data security and human rights can be violated as there arent laws regulating the use of AI or ML models for police, goverments and business.\n",
    "\n",
    "What we can do is keep all this in mind to create AI and ML models that try to asses these issues, like Aya, a multi-languange-ML AI (can translate to 101 languages besides english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa94ae0-a059-461b-8ba1-7d8c5bbf6f1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tutorial for git and conda by Assistant profesors Martina Cadiz (Universidad de Chile) and Nicol√°s Monsalves (Universidad de la Serena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c716962-3985-480a-8cb8-768edcbf6298",
   "metadata": {},
   "source": [
    "There's going to be a github repository for us to see notes, exercises, theretical notes, etc. Check https://github.com/lssds2024/lssds2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b6a53-87b4-4962-b5f8-89e6958f6115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Projects and groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d9b72-2aaa-4ad0-9ce1-6865533aa30b",
   "metadata": {},
   "source": [
    "Group by 4 people, create a title for the group, and decide as a group which proyects we want\n",
    "\n",
    "Group5 : Name to be decided soon\n",
    "- Karina Barboza\n",
    "- Ian Espinal\n",
    "- Nicolas Henriquez\n",
    "- and me <3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d5422-b628-43da-9d95-cdc52ecd631b",
   "metadata": {},
   "source": [
    "PROYECT 1: will AI replace astronomers?, Create an AI astronomer that can answer stuff about a light curve, the important thing is the visual question answering model.\n",
    "\n",
    "BLIP-2 models.\n",
    "\n",
    "\n",
    "Mathew Pattol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9ae3c-1789-4675-a1df-c95853064779",
   "metadata": {},
   "source": [
    "PROYECT 2: Find exocomets by emission lines by the optically thin material emerging by the comet near a star. \n",
    "\n",
    "\n",
    "Amelia Bayo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca407c28-5b35-4de7-8ef3-01559f061cf2",
   "metadata": {},
   "source": [
    "PROYECT 3: Create a unsupervised ML model for ESO papers to be recognize and related (map of related papers), but there are some problems with clustering papers as words like ALMA can mean a lot of things.\n",
    "\n",
    "\n",
    "Use models like BERT, AstroBERT, etc. (Grezers et al., 2021)\n",
    "\n",
    "Amelia Bayo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f6bf0-f173-456b-997c-15c06c9f7c05",
   "metadata": {},
   "source": [
    "PROYECT 4: Kolmogorov-Arnold Networks (KANs) a new neural network. Find photometric redshifts with KANs and compare them to the other solutions that were acceptable before. (LIKEY)\n",
    "\n",
    "\n",
    "Matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014644fa-90b0-4c52-82a0-cc966f8803c8",
   "metadata": {},
   "source": [
    "PROYECT 5: Explore using sonification techniques to indentify different aural patterns for types of transients (SNovae) and compare against visual techniques.\n",
    "\n",
    "Matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f36e07-67ba-4149-a820-23b2ae4eb3ba",
   "metadata": {},
   "source": [
    "PROYECT 6: Deploy a service like CHATGPT with information of something, to be able to access the data ina easier way.\n",
    "\n",
    "Focused on middleware, the data exploration part of the data, and the engineering behind the modeling part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f1773-e82f-42d6-aa1e-c3a26525144a",
   "metadata": {},
   "source": [
    "PROYECT 7: White matter hyperintensities segmentation in MRI images, create a model that can identify the white spots and classify if the damage is important or not. (damage quantifycation)\n",
    "\n",
    "Mauricio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fed12-6798-45e6-8e0a-80c1d5f19a94",
   "metadata": {},
   "source": [
    "PROYECT 8: Refining YSO classification from ALeRCE, classify the alerts in different objects and which parameters gives you that. We need to improve the broker and add more classification for YSO. (taxonomy)\n",
    "\n",
    "Amelia Bayo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73d603-4c29-4676-967f-15453b9e84f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Unsupervised and supervised machine learning by Amelia Bayo ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407303d9-c7be-4b71-af0f-5cf538853fb4",
   "metadata": {},
   "source": [
    "Spectroscopy: The study of emission and absorption lines in difracted light.\n",
    "\n",
    "Talked about a catalog, and the importance of it. In the astronomical world the same source can have multiple names, causing confusion within astronomers. Then catalogs that can correlate the names, coordinates, etc to a source, this is not a easy task\n",
    "\n",
    "Remember to always check for NaN values but also for funny characters, these are values that do not make sense! So we have to erase them, or work with them looking for why these funny numbers happend.\n",
    "\n",
    "------\n",
    "\n",
    "Why are histograms complicated? The size of the bins and where do we start counting can affect how the histogram look. Also the histogram assume that every detection is exactly the source. But if we change to gaussian sources we can create kernel density histograms, which would tell us more about the data and the uncertainties.\n",
    "\n",
    "Remember that as for example, we can use Gaussian Mixed models, with N numbers of gaussians and N numbers of dimensions, but i can make anything with enough gaussians, then we need to penalize the model to avoid overfitting the data. This penalization can be make by BIC (bayesian information criteria) or AIC (Aikake information criterion)\n",
    "\n",
    "The idea of the penalization is that the more parameters the model has, more the model has to pay attention for overfitting, then it is kind of an exponential thing. The chi squared or R squared parameters are linear, then the number of parameters involve does not affect this number.\n",
    "\n",
    "----\n",
    "Back to emission lines, the broadening is the rotation velocity of the star, as some parts are coming closer to us and another part moving away, making the thin line broad.\n",
    "\n",
    "But sometimes theres going to be to many components, we need to reduce dimentionality (PCA), always thinking to maintain the information (variance). this turn the data to be weighted by information, regardless of the mean value in such axis(parameters)\n",
    "\n",
    "Then sometimes, the data tend to cluster itself, so we want models that can find this clustering. KMeans check the distance to centers according to other point of the data, but the edges are not well behave and also outliers are not declared or accounted for. Besides, it is sensitive to initial guesses.\n",
    "\n",
    "Then there are more methods more adequate for it. Like HDBSCAN. If the model can not find any cluster (theres just 1) this means that the data is just noise, and there ir no clustering, it does not mean anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8eaae-0cbc-48e2-b501-dacb790eef94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Supervised classification by Ashish Mahabal (Caltech ashish@caltech.edu) and Amelia Bayo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0492e3-bc3a-4738-9f54-e600cc2193fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "When we start a classification problem, we start with thunking about the taxonomy what classes do we have? How many are they?\n",
    "\n",
    "And we need to alwasy considers artifacts, CCD defects, etc. \n",
    "\n",
    "-------\n",
    "\n",
    "complications:\n",
    "- Add clasess's behaviours in the model\n",
    "- edges\n",
    "- deciding the classifier (XGBoost, RandomForest, Neural Networks, etc), the interpretability of the parameters affects this decision\n",
    "- Aspects of training data\n",
    "\n",
    "\n",
    "----------\n",
    "Define a scoring function, to know how well are we doing the model.\n",
    "\n",
    "----\n",
    "- we want the triningt set to be balance, check mean and variance for that\n",
    "\n",
    "- for a classification problem if i want to check how well model is, i need to study its metrics, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714f97e-7de1-41d9-948f-0b11f0c3bf1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## August 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d5a1e-d6ec-41e9-bd79-6e44358cbd6a",
   "metadata": {},
   "source": [
    "### Deep Learning by Matthew Graham () and another Matthew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106162-e01f-446c-bb69-26541a85bf17",
   "metadata": {},
   "source": [
    "The idea about deep learning is about building a network, just like biological neural network. \n",
    "\n",
    "---\n",
    "The perceptron\n",
    "\n",
    "The fisrt computational model of a neuron(1943), the model consists in adding the inputs until a certain threshold and if that happens then the neuron pass the information. THe outputs are just 1 or 0, like a light switch.\n",
    "\n",
    "In 1958, weights were added to the perception inputs, then this weighted inputs are computed through an ACTIVATION FUNCTION, this determine the output. (Here also bias is defined as something to linearly move the activation function)\n",
    "\n",
    "Whats an appropiate AF?\n",
    "The heaviside is the easir one but has problem as it doesnt have define derivatives, then sigmoid, ReLU, and Tanh are more common and continous fie derivation.\n",
    "\n",
    "----\n",
    "But we need to see how well does our neuron work, then LOSS FUNCTION are introduced.\n",
    "we have L2 loss (mse), and L1 loss (absolute error)\n",
    "To see if the neuron is sensituve to weights and its changes, we want to minimilize the loss ufnction with the ideal set of weights. Then we study the gradient, and if we find the gobal minimum.\n",
    "\n",
    "THen the weights must be a function instead of a discrete value.\n",
    "\n",
    "But this looks for linearly problems, and whenever the logic problem is not linearly separable we need to add more layers, with nonlinear activation functions. \n",
    "\n",
    "----\n",
    "\n",
    "In theory only three layers must be enough to find any solution but more layers my be useful.\n",
    "How can we connect them? Backpropagation!\n",
    "\n",
    "Conceptually, the idea of backpropagation is that we first forward propagate through every layer of the network to get the predicted value and calculate the value of the loss function. We then backpropagate this across every layer in reverse order updating the weights by the appropriate delta (update) as we proceed. So if a network has 5 layers, to calculate the backpropagated error for layer 1, we would first forward propagate from layer 1 => layer 2 => ... => layer 5 => output activation and then backpropagate from output activation => layer 5 => layer 4 => ... => layer 1 and then apply this delta to update the weights of every node in layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd621f-ed89-466f-9a87-1624f65a5444",
   "metadata": {},
   "source": [
    "REGULARIZATION\n",
    "\n",
    "The idea is to avoid overfitting, where the accuracy in the training set is lower but higher at the test set.\n",
    "\n",
    "then we can avoir overfitting using Early Stoppage, you create a validaation set inside the training set and on the validation set, as the model is training, it check itself the accuracy or rms and if it is bad, stop.\n",
    "\n",
    "ALso, theres the dropout layer that can be added, so then i randomly remove some of the nodes in the network.\n",
    "\n",
    "Batch size, number oof training isnstances the network evaluates per weight update step. Larger batches gives more computational speed but a lower one give more generalization.\n",
    "\n",
    "DO NOT USE BATCHES LARGER THAN 32\n",
    "\n",
    "-----\n",
    "OPtimizer\n",
    "\n",
    "ADDAM: Gradiant descent to find the minimal thingy\n",
    "\n",
    "-------\n",
    "\n",
    "COST/LOST FUNCTIONS\n",
    "\n",
    "Regression, find a numerical value(linear activation and L2 loss function )\n",
    "BInary classification, classify into 2 classes(SIGMOID ACTIVATION CROSS ENTROPY AS LOSS FUNCTION) \n",
    "multuclass classification, more than 2 classes (softmax activation function and cross entropy as your loss function)\n",
    "\n",
    "------\n",
    "\n",
    "There are more types of neural networks :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252cef7-e715-4d23-8d30-aa1ca30d4876",
   "metadata": {},
   "source": [
    "### Manifold learning and dimesionaloity reduction by Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0fd4e-8b6f-4f52-b115-a9bbb26b0cbf",
   "metadata": {},
   "source": [
    "Dimesionality reduction\n",
    "\n",
    "WE want to make this beacuse the visulization is diffciult for higher dimention dataset, some parameters may be reductant and the distances are not well define between dimentions, so interpretability is hard.\n",
    "\n",
    "To asses the visualization problem we can use t-SNE methods, the perplecity is how many neighbors are we goinf¬øg to consider, and then the early exxaggeration it stracthed the distances. (This thing has problem with its loss function, thats why it is stochastic, it changed a lot if we change the hyperparameter.\n",
    "\n",
    "Visualization is important because, we mostly need simmulations to be precise, but that cost to much, so we can train models to take low resolution simmulations and create the high resolution one.\n",
    "\n",
    "--------\n",
    "AUTOENCODER\n",
    "\n",
    "Whe have a lot of layers, and then a bottleneck, to minimal neuron, then back to the whole size. With this I kind recreate an image, we encode the data and then decode it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186745a-7a82-413e-a1b6-fa4b2769cc47",
   "metadata": {},
   "source": [
    "## August 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b374b5-8b6f-4445-a716-9d69e35c9e45",
   "metadata": {},
   "source": [
    "### Image processing by Mauricio Cerda (mauriciocerda@med.uchile.cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3288ba-12df-4c5f-bd2e-5ba63315e0a8",
   "metadata": {},
   "source": [
    "WHenever we want to use images, we can go for Spatial descriptions or temporal descriptions.\n",
    "\n",
    "the depth if the image is 8 bytes. 255 is max number ypu can represent in RGB.\n",
    "\n",
    "A techniqe for segmentation is a threshold filter, to decide a threshold we can see the histogram of the image and separate populations from it.\n",
    "\n",
    "Another one is convolution, keeeping the original image size, we need a kernel. After convoution I can focus on some properties of the image and then segment better.\n",
    "\n",
    "We can algo dilate or erode images or make a mix of that.\n",
    "\n",
    "-----\n",
    "\n",
    "Moments of morphology\n",
    "\n",
    "THe variance and covariance of an image helps us to find the segmentation image size, lika PCA. The idea is to find the principal componets, semimajor axis, or something like that.\n",
    "\n",
    "-----\n",
    "\n",
    "Sometimes just a simple threshold doenst work, as it is normal for images to have gradients.\n",
    "then we need to change to machine learning algorithms, then we can use the brightness of the original image with a filtered one, and compare in a feature space. And then treat the problem as a classification, regresion or cluster one.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfd06c-ff80-465a-9efd-1aa18ae3940b",
   "metadata": {},
   "source": [
    "### Generative adversarial Network GANs by Pavlos Protopapas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01a41d-2ab9-42b9-898b-2cace59beaa2",
   "metadata": {},
   "source": [
    "Generative modeling: create images\n",
    "\n",
    "steps: \n",
    "- Pure noise (the reason is that is have to start with stochastic variable, some random variable)\n",
    "- \n",
    "\n",
    "----\n",
    "\n",
    "EXplicit sampling: form an analytical exression for the model, like MCMC (good but high consuming, inverse transform sampling(works only if we can integrate the expression) and variational methods(convert to optimization problem).\n",
    "But also implciit one, without the analytical expression, GAN, generator part of VAE(variotional autoencoder), probabilistic diffusion models.\n",
    "\n",
    "----\n",
    "why do we need GANs?\n",
    "\n",
    "Relaistic generaatio task, data augmenetation, missing data, simulation and planning.\n",
    "\n",
    "----\n",
    "how does it work?\n",
    "\n",
    "we have a generator who gave negative data, and a discriminator who tries to find the negative data in the dataset. \n",
    "\n",
    "------\n",
    "Arquitecture\n",
    "\n",
    "ssample ----> discriminator ----> pbbb\n",
    "we add the random generator, then sample ---> generator ---> distribution we want to emulate\n",
    "\n",
    "then\n",
    "sample_true  + sample false ---> genetator ---> pbbb\n",
    "\n",
    "remember that this is a binary classification problem, then the loss function for GANS is the binary cros entrrop loss.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
